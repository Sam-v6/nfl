{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1df8838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       gameId  playId    nflId     displayName  frameId    frameType  \\\n",
      "0  2022091200      64  35459.0  Kareem Jackson        1  BEFORE_SNAP   \n",
      "1  2022091200      64  35459.0  Kareem Jackson        2  BEFORE_SNAP   \n",
      "2  2022091200      64  35459.0  Kareem Jackson        3  BEFORE_SNAP   \n",
      "3  2022091200      64  35459.0  Kareem Jackson        4  BEFORE_SNAP   \n",
      "4  2022091200      64  35459.0  Kareem Jackson        5  BEFORE_SNAP   \n",
      "5  2022091200      64  35459.0  Kareem Jackson        6  BEFORE_SNAP   \n",
      "6  2022091200      64  35459.0  Kareem Jackson        7  BEFORE_SNAP   \n",
      "7  2022091200      64  35459.0  Kareem Jackson        8  BEFORE_SNAP   \n",
      "8  2022091200      64  35459.0  Kareem Jackson        9  BEFORE_SNAP   \n",
      "9  2022091200      64  35459.0  Kareem Jackson       10  BEFORE_SNAP   \n",
      "\n",
      "                    time  jerseyNumber club playDirection      x      y     s  \\\n",
      "0  2022-09-13 00:16:03.5          22.0  DEN         right  51.06  28.55  0.72   \n",
      "1  2022-09-13 00:16:03.6          22.0  DEN         right  51.13  28.57  0.71   \n",
      "2  2022-09-13 00:16:03.7          22.0  DEN         right  51.20  28.59  0.69   \n",
      "3  2022-09-13 00:16:03.8          22.0  DEN         right  51.26  28.62  0.67   \n",
      "4  2022-09-13 00:16:03.9          22.0  DEN         right  51.32  28.65  0.65   \n",
      "5    2022-09-13 00:16:04          22.0  DEN         right  51.37  28.68  0.62   \n",
      "6  2022-09-13 00:16:04.1          22.0  DEN         right  51.43  28.71  0.61   \n",
      "7  2022-09-13 00:16:04.2          22.0  DEN         right  51.47  28.75  0.61   \n",
      "8  2022-09-13 00:16:04.3          22.0  DEN         right  51.52  28.80  0.62   \n",
      "9  2022-09-13 00:16:04.4          22.0  DEN         right  51.56  28.84  0.61   \n",
      "\n",
      "      a   dis       o    dir                 event  \n",
      "0  0.37  0.07  246.17  68.34  huddle_break_offense  \n",
      "1  0.36  0.07  245.41  71.21                   NaN  \n",
      "2  0.23  0.07  244.45  69.90                   NaN  \n",
      "3  0.22  0.07  244.45  67.98                   NaN  \n",
      "4  0.34  0.07  245.74  62.83                   NaN  \n",
      "5  0.40  0.06  247.01  59.35                   NaN  \n",
      "6  0.42  0.06  248.57  55.39                   NaN  \n",
      "7  0.49  0.06  251.03  48.20                   NaN  \n",
      "8  0.46  0.06  253.26  43.11                   NaN  \n",
      "9  0.40  0.06  254.75  39.52                   NaN  \n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Load data\n",
    "#################################################################\n",
    "\n",
    "import os\n",
    "os.environ[\"NFL_HOME\"] = \"/home/sam/repos/hobby-repos/nfl/\"\n",
    "\n",
    "from common.data_loader import DataLoader\n",
    "\n",
    "# Get raw data\n",
    "loader = DataLoader()\n",
    "games_df, plays_df, players_df, location_data_df = loader.get_data(weeks=[week for week in range (1,10)])\n",
    "\n",
    "print(location_data_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5daad0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering data...\n",
      "Total plays: 16124\n",
      "Total plays after filtering out penalties: 15908\n",
      "Total plays after filtering to valid Man or Zone classifications: 14910\n",
      "Total plays after filtering to only pass plays: 9103\n",
      "Total plays after filtering out garbage time: 7079\n",
      "Total plays after filtering for 3rd or 4th down: 2028\n",
      "Total plays after making sure they are in our location data: 2028\n",
      "Index(['gameId', 'playId', 'playDescription', 'quarter', 'down', 'yardsToGo',\n",
      "       'possessionTeam', 'defensiveTeam', 'yardlineSide', 'yardlineNumber',\n",
      "       'gameClock', 'preSnapHomeScore', 'preSnapVisitorScore',\n",
      "       'playNullifiedByPenalty', 'absoluteYardlineNumber',\n",
      "       'preSnapHomeTeamWinProbability', 'preSnapVisitorTeamWinProbability',\n",
      "       'expectedPoints', 'offenseFormation', 'receiverAlignment',\n",
      "       'playClockAtSnap', 'passResult', 'passLength', 'targetX', 'targetY',\n",
      "       'playAction', 'dropbackType', 'dropbackDistance', 'passLocationType',\n",
      "       'timeToThrow', 'timeInTackleBox', 'timeToSack', 'passTippedAtLine',\n",
      "       'unblockedPressure', 'qbSpike', 'qbKneel', 'qbSneak',\n",
      "       'rushLocationType', 'penaltyYards', 'prePenaltyYardsGained',\n",
      "       'yardsGained', 'homeTeamWinProbabilityAdded',\n",
      "       'visitorTeamWinProbilityAdded', 'expectedPointsAdded', 'isDropback',\n",
      "       'pff_runConceptPrimary', 'pff_runConceptSecondary', 'pff_runPassOption',\n",
      "       'pff_passCoverage', 'pff_manZone'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Filter down plays df to situations we are interested in\n",
    "#################################################################\n",
    "filtered_plays_df = plays_df.copy()\n",
    "\n",
    "# Find the starting plays\n",
    "print(\"Filtering data...\")\n",
    "original_play_length = len(filtered_plays_df)\n",
    "print(f'Total plays: {original_play_length}')\n",
    "\n",
    "# Filter out penalties\n",
    "filtered_plays_df = filtered_plays_df[filtered_plays_df['playNullifiedByPenalty'] == 'N']\n",
    "# Filter out rows with 'PENALTY' in the 'playDescription' column\n",
    "filtered_plays_df = filtered_plays_df[~filtered_plays_df['playDescription'].str.contains(\"PENALTY\", na=False)]\n",
    "print(f'Total plays after filtering out penalties: {len(filtered_plays_df)}')\n",
    "\n",
    "# Filter down to valid Man or Zone defensive play calls\n",
    "filtered_plays_df = filtered_plays_df[filtered_plays_df['pff_manZone'].isin(['Man', 'Zone'])]\n",
    "print(f'Total plays after filtering to valid Man or Zone classifications: {len(filtered_plays_df)}')\n",
    "\n",
    "# Filter for only rows that indicate a pass play\n",
    "filtered_plays_df = filtered_plays_df[filtered_plays_df['passResult'].notna()]\n",
    "print(f'Total plays after filtering to only pass plays: {len(filtered_plays_df)}')\n",
    "\n",
    "# Filter for only plays where the win probablity isn't lopsided (between 0.2 and 0.8)\n",
    "filtered_plays_df = filtered_plays_df[(filtered_plays_df['preSnapHomeTeamWinProbability'] > 0.1) & (filtered_plays_df['preSnapHomeTeamWinProbability'] < 0.9)]\n",
    "print(f'Total plays after filtering out garbage time: {len(filtered_plays_df)}')\n",
    "\n",
    "# Filter for only third down or fourth down plays\n",
    "filtered_plays_df = filtered_plays_df[filtered_plays_df['down'].isin([3, 4])]\n",
    "print(f'Total plays after filtering for 3rd or 4th down: {len(filtered_plays_df)}')\n",
    "\n",
    "# Filter for plays that are in our gameIds (in location data df)\n",
    "filtered_plays_df = filtered_plays_df[filtered_plays_df['gameId'].isin(location_data_df['gameId'].unique())]\n",
    "print(f'Total plays after making sure they are in our location data: {len(filtered_plays_df)}')\n",
    "\n",
    "print(filtered_plays_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ccf989eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total plays after cutting down to our cols and dropping NAs: 2028\n",
      "        gameId  playId possessionTeam defensiveTeam pff_manZone\n",
      "9   2022100204    1422            DAL           WAS        Zone\n",
      "10  2022100912     445            DAL            LA        Zone\n",
      "16  2022100913    2604            CIN           BAL        Zone\n",
      "20  2022092505    3437             NE           BAL         Man\n",
      "30  2022091803    1222            IND           JAX         Man\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Cut down df to only cols we care about\n",
    "#################################################################\n",
    "# Cut down to columns we care about\n",
    "keep_cols_from_plays = ['gameId', 'playId', 'possessionTeam', 'defensiveTeam', 'pff_manZone']\n",
    "filtered_plays_df = filtered_plays_df.loc[:, keep_cols_from_plays].drop_duplicates()\n",
    "\n",
    "# Make sure we don't have any NAs in this cut down col df\n",
    "filtered_plays_df.dropna()\n",
    "print(f'Total plays after cutting down to our cols and dropping NAs: {len(filtered_plays_df)}')\n",
    "\n",
    "# Print total plays\n",
    "print(filtered_plays_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f12967ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       gameId  playId    nflId  frameId    frameType club      x      y\n",
      "0  2022091200      64  35459.0        1  BEFORE_SNAP  DEN  51.06  28.55\n",
      "1  2022091200      64  35459.0        2  BEFORE_SNAP  DEN  51.13  28.57\n",
      "2  2022091200      64  35459.0        3  BEFORE_SNAP  DEN  51.20  28.59\n",
      "3  2022091200      64  35459.0        4  BEFORE_SNAP  DEN  51.26  28.62\n",
      "4  2022091200      64  35459.0        5  BEFORE_SNAP  DEN  51.32  28.65\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Create merged df that has gameId, playId, frameID all before SNAP, with x, y, and offense/defense\n",
    "#################################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a copy of the location tracking data, cut it down to columns we care about\n",
    "loc_trimmed_df = location_data_df.copy()\n",
    "keep_cols  = [\n",
    "    'gameId',\n",
    "    'playId',\n",
    "    'nflId',\n",
    "    'frameId',\n",
    "    'frameType',\n",
    "    'club',\n",
    "    'x',\n",
    "    'y',\n",
    "]\n",
    "loc_trimmed_df = location_data_df.loc[:, keep_cols]\n",
    "\n",
    "# Cut down location tracking data copy to only before the snap and where the team isn't valid\n",
    "loc_trimmed_df = loc_trimmed_df[(loc_trimmed_df[\"frameType\"] == \"BEFORE_SNAP\") & (loc_trimmed_df[\"club\"] != \"football\")]\n",
    "\n",
    "print(loc_trimmed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7282842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             gameId  playId pff_manZone    nflId  frameId      x      y side\n",
      "4264920  2022090800     236        Zone  35472.0        1  38.21  28.48  off\n",
      "4265012  2022090800     236        Zone  38577.0        1  31.07  30.45  def\n",
      "4265104  2022090800     236        Zone  41239.0        1  31.67  29.14  def\n",
      "4265196  2022090800     236        Zone  42392.0        1  38.08  29.51  off\n",
      "4265288  2022090800     236        Zone  42816.0        1  29.85  28.78  def\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Label offense/defense, cut down df further, and sort\n",
    "#################################################################\n",
    "# Merge the two datasets such that we can have the possession and defensive team for each row\n",
    "merged_df = pd.merge(filtered_plays_df, loc_trimmed_df, on=['gameId', 'playId'], how='inner')\n",
    "\n",
    "# Tag the \"side\" of the player for each row (that being \"off\" or \"def\")\n",
    "merged_df['side'] = np.where(merged_df['club'] == merged_df['possessionTeam'], 'off', 'def')\n",
    "\n",
    "# Drop some columns we don't need anymore\n",
    "merged_df = merged_df.drop(['possessionTeam', 'defensiveTeam', 'club', 'frameType'], axis=1)\n",
    "\n",
    "# Sort for deterministic frame ordering\n",
    "merged_df = merged_df.sort_values(['gameId','playId','frameId'])\n",
    "\n",
    "# Let's see what we have\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a864635a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using plays that have above 70 frames\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Decide the target sequence length using the median number of frames per play\n",
    "#################################################################\n",
    "frame_counts = (merged_df\n",
    "                .groupby(['gameId','playId'])['frameId']\n",
    "                .nunique())\n",
    "min_frames = int(np.percentile(frame_counts.values, 10))\n",
    "print(f\"Using plays that have above {min_frames} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "21371d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept plays: 1826\n",
      "Skipped (>11 players): 0\n",
      "Skipped (<70 frames): 202\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "# Built dataset where frame is only min frames and each side has exactly 11 players\n",
    "#################################################################\n",
    "def exactly_eleven_per_side(play_df: pd.DataFrame) -> bool:\n",
    "    return (\n",
    "        play_df.loc[play_df.side == 'off', 'nflId'].nunique() == 11 and\n",
    "        play_df.loc[play_df.side == 'def', 'nflId'].nunique() == 11\n",
    "    )\n",
    "\n",
    "def slot_order_by_left_to_right(play_df: pd.DataFrame, side: str) -> list:\n",
    "    side_df = play_df.loc[play_df[\"side\"] == side]\n",
    "    stats = (side_df\n",
    "             .groupby('nflId', as_index=True)[['x','y']]\n",
    "             .median()\n",
    "             .rename(columns={'x':'x_med','y':'y_med'})\n",
    "             .sort_values(['x_med','y_med']))\n",
    "    return stats.index.tolist()  # list of sorted NFL player ids for this play to determine median x --> y player locs\n",
    "\n",
    "def build_side_xy_cube(play_df: pd.DataFrame, side: str, frames: np.ndarray) -> np.ndarray:\n",
    "    # pivot to (frames x slots) for x and y, fill missing with NaN, then stack → (min_frames, 11, 2)\n",
    "    side_df = play_df.loc[play_df[\"side\"] == side]\n",
    "\n",
    "\n",
    "    # Takes the long df and goes from frameId, slot, x, y as cols to:\n",
    "    # slot, 0, 1, 2 as cols ... with frameId 1, frameId 2... etc as the rows....shape is (min_frames, 11)\n",
    "    x_mat = side_df.pivot_table(index='frameId', columns='slot', values='x')\n",
    "    y_mat = side_df.pivot_table(index='frameId', columns='slot', values='y')\n",
    "\n",
    "\n",
    "    # It's possible certain players don't have exact tracking data throughout (ie one player has frame 10 and 12 but not frame 11), this will end up breaking our shape and cause issues downstream for model training\n",
    "    # So this forces the matrix to have for each frame \n",
    "    x_mat = x_mat.reindex(index=frames, columns=range(11), fill_value=np.nan)\n",
    "    y_mat = y_mat.reindex(index=frames, columns=range(11), fill_value=np.nan)\n",
    "\n",
    "    # Stack along last axis such that we return (min_frames, 11, 2)\n",
    "    return np.stack([x_mat.to_numpy(), y_mat.to_numpy()], axis=-1)\n",
    "\n",
    "# Init series maps\n",
    "off_series = {}\n",
    "def_series = {}\n",
    "\n",
    "# Lists to peek at later if we skip plays\n",
    "skipped_wrong_player_count_list = []   # plays where offense or defense had >11 unique players\n",
    "skipped_under_min_frames_list = []     # plays with fewer than min_frames\n",
    "\n",
    "# Iterate on each play\n",
    "for (game_id, play_id), play in merged_df.groupby(['gameId','playId'], sort=False):\n",
    "    # Skip if not 11 players\n",
    "    if not exactly_eleven_per_side(play):\n",
    "        skipped_wrong_player_count_list.append((game_id, play_id))\n",
    "        continue\n",
    "\n",
    "    # Define slot maps (left→right by median x, tie-break median y)\n",
    "    off_slots = slot_order_by_left_to_right(play, 'off')\n",
    "    def_slots = slot_order_by_left_to_right(play, 'def')\n",
    "\n",
    "    # Create a map that goes player id --> index so we can assign each player to an index as we go frame by frame\n",
    "    off_id2slot = {pid: i for i, pid in enumerate(off_slots)}\n",
    "    def_id2slot = {pid: i for i, pid in enumerate(def_slots)}\n",
    "\n",
    "    # Assign slots (if offense use offensive map, if defense, use defensive map)\n",
    "    tmp = play.copy()\n",
    "    tmp['slot'] = np.where(\n",
    "        tmp['side'] == 'off',\n",
    "        tmp['nflId'].map(off_id2slot),\n",
    "        tmp['nflId'].map(def_id2slot)\n",
    "    )\n",
    "\n",
    "    # Choose frame window (last min_frames frames)\n",
    "    frames_all = np.sort(tmp['frameId'].unique())\n",
    "    if frames_all.size < min_frames:\n",
    "        skipped_under_min_frames_list.append((game_id, play_id))\n",
    "        continue\n",
    "    frames = frames_all[-min_frames:]  # Get the last min frames, so each play is consistent\n",
    "\n",
    "    # Build offense/defense cubes: (min_frames, 11, 2) the 2 is x and y coords\n",
    "    off_arr = build_side_xy_cube(tmp, 'off', frames)\n",
    "    def_arr = build_side_xy_cube(tmp, 'def', frames)\n",
    "\n",
    "    off_series[(game_id, play_id)] = off_arr\n",
    "    def_series[(game_id, play_id)] = def_arr\n",
    "\n",
    "print(f\"Kept plays: {len(off_series)}\")\n",
    "print(f\"Skipped (>11 players): {len(skipped_wrong_player_count_list)}\")\n",
    "print(f\"Skipped (<{min_frames} frames): {len(skipped_under_min_frames_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd0c4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Data functions to build dataset with imputing and converting to tensors\n",
    "#################################################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def impute_timewise(X_np: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    X_np: (T, F) with NaNs.\n",
    "    Impute per feature (column) along time:\n",
    "      1) forward-fill if we miss a frame, assume the player stayed where he was last seen.\n",
    "      2) back-fill\n",
    "      3) fill remaining NaNs with column mean (0 if all NaN)\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(X_np)                 # (T, 11 players * 2 features = 44)\n",
    "\n",
    "    # Copy the last known values fwd in time if there's missing NaNs\n",
    "    # Fill any leading NaNs that had no earlier data\n",
    "    # Example: [NaN, 3, 4, NaN, NaN, 7] ---> foward fill [NaN, 3, 4, 4, 4, 7] ---> backward fill [3, 3, 4, 4, 4, 7]\n",
    "    # If a whole column has NaNs we then fill it with 0s (only time this realistically kicks in)\n",
    "    df = df.ffill().bfill().fillna(0.0)\n",
    "\n",
    "\n",
    "    return df.values.astype(np.float32)\n",
    "\n",
    "def build_plays_data(off_series, def_series, labels_dict):\n",
    "    X, y = [], []\n",
    "    for key, off_arr in off_series.items():\n",
    "        def_arr = def_series[key]\n",
    "        X_play = np.concatenate([off_arr, def_arr], axis=1).reshape(off_arr.shape[0], -1)\n",
    "        X_play = impute_timewise(X_play)\n",
    "        X.append(torch.from_numpy(X_play).float())\n",
    "        y.append(torch.tensor(labels_dict[key], dtype=torch.long))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8a0c6769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Build labels dict mapping of (gameId, playId) --> 0/1\n",
    "label_map = {'Man': 1, 'Zone': 0}\n",
    "labels_dict = {(r.gameId, r.playId): label_map[r.pff_manZone] for r in filtered_plays_df.itertuples()}\n",
    "\n",
    "# Get dataset (imputed, converted to tensor)\n",
    "X, y = build_plays_data(off_series, def_series, labels_dict)\n",
    "\n",
    "# Splittys\n",
    "idx_train, idx_val = train_test_split(\n",
    "    np.arange(len(X)),                  # Create an array from 0 to x number of plays\n",
    "    test_size=0.2,                      # Choosing standard 20% for test size\n",
    "    random_state=42,                    # Life universe and everything\n",
    "    stratify=[yy.item() for yy in y]    # Takes each small tensor (these are small scalar tensors) of our labels, converts to 0 or 1, then makes a list of the labels [0, 0, 1, 0, 1,...] --> says split the data while keeping same ratio of 0s and 1s in both train and validation sets\n",
    "    )\n",
    "\n",
    "# Make tensor datasets\n",
    "# NOTE: X will be of shape (play_count, min_frames, 44)\n",
    "# NOTE: Y will be of shape (play_count, )\n",
    "train_ds = TensorDataset(torch.stack([X[i] for i in idx_train]),    # Each x is (min_frame, 44)\n",
    "                         torch.stack([y[i] for i in idx_train]))\n",
    "val_ds   = TensorDataset(torch.stack([X[i] for i in idx_val]),\n",
    "                         torch.stack([y[i] for i in idx_val]))\n",
    "\n",
    "# Reproducibility seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# For deterministic behavior (slower, optional)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Make dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "babd21e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size=44, hidden_size=64, num_layers=1, dropout=0.0, bidir=False, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=bidir,\n",
    "        )\n",
    "        out_dim = hidden_size * (2 if bidir else 1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(out_dim),\n",
    "            nn.Linear(out_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (B, T, F)\n",
    "        out, (h_n, c_n) = self.lstm(x)        # out: (B, T, H)\n",
    "        last = out[:, -1, :]                   # use last timestep representation\n",
    "        logits = self.head(last)               # (B, C)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "31f7b161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights (Zone, Man): [0.90458488 1.1179173 ]\n",
      "Epoch 1: val acc = 0.495\n",
      "Epoch 2: val acc = 0.443\n",
      "Epoch 3: val acc = 0.456\n",
      "Epoch 4: val acc = 0.508\n",
      "Epoch 5: val acc = 0.530\n",
      "Epoch 6: val acc = 0.527\n",
      "Epoch 7: val acc = 0.579\n",
      "Epoch 8: val acc = 0.484\n",
      "Epoch 9: val acc = 0.470\n",
      "Epoch 10: val acc = 0.522\n",
      "Epoch 11: val acc = 0.464\n",
      "Epoch 12: val acc = 0.546\n",
      "Epoch 13: val acc = 0.473\n",
      "Epoch 14: val acc = 0.549\n",
      "Epoch 15: val acc = 0.470\n",
      "Epoch 16: val acc = 0.582\n",
      "Epoch 17: val acc = 0.489\n",
      "Epoch 18: val acc = 0.464\n",
      "Epoch 19: val acc = 0.484\n",
      "Epoch 20: val acc = 0.467\n",
      "Epoch 21: val acc = 0.568\n",
      "Epoch 22: val acc = 0.503\n",
      "Epoch 23: val acc = 0.519\n",
      "Epoch 24: val acc = 0.574\n",
      "Epoch 25: val acc = 0.579\n",
      "Epoch 26: val acc = 0.533\n",
      "Epoch 27: val acc = 0.536\n",
      "Epoch 28: val acc = 0.549\n",
      "Epoch 29: val acc = 0.467\n",
      "Epoch 30: val acc = 0.459\n",
      "Epoch 31: val acc = 0.462\n",
      "Epoch 32: val acc = 0.464\n",
      "Epoch 33: val acc = 0.522\n",
      "Epoch 34: val acc = 0.571\n",
      "Epoch 35: val acc = 0.568\n",
      "Epoch 36: val acc = 0.464\n",
      "Epoch 37: val acc = 0.519\n",
      "Epoch 38: val acc = 0.459\n",
      "Epoch 39: val acc = 0.475\n",
      "Epoch 40: val acc = 0.500\n",
      "Epoch 41: val acc = 0.503\n",
      "Epoch 42: val acc = 0.511\n",
      "Epoch 43: val acc = 0.475\n",
      "Epoch 44: val acc = 0.505\n",
      "Epoch 45: val acc = 0.459\n",
      "Epoch 46: val acc = 0.525\n",
      "Epoch 47: val acc = 0.511\n",
      "Epoch 48: val acc = 0.544\n",
      "Epoch 49: val acc = 0.522\n",
      "Epoch 50: val acc = 0.563\n",
      "Epoch 51: val acc = 0.451\n",
      "Epoch 52: val acc = 0.500\n",
      "Epoch 53: val acc = 0.522\n",
      "Epoch 54: val acc = 0.511\n",
      "Epoch 55: val acc = 0.530\n",
      "Epoch 56: val acc = 0.514\n",
      "Epoch 57: val acc = 0.470\n",
      "Epoch 58: val acc = 0.522\n",
      "Epoch 59: val acc = 0.475\n",
      "Epoch 60: val acc = 0.492\n",
      "Epoch 61: val acc = 0.464\n",
      "Epoch 62: val acc = 0.533\n",
      "Epoch 63: val acc = 0.566\n",
      "Epoch 64: val acc = 0.475\n",
      "Epoch 65: val acc = 0.467\n",
      "Epoch 66: val acc = 0.522\n",
      "Epoch 67: val acc = 0.533\n",
      "Epoch 68: val acc = 0.533\n",
      "Epoch 69: val acc = 0.574\n",
      "Epoch 70: val acc = 0.492\n",
      "Epoch 71: val acc = 0.527\n",
      "Epoch 72: val acc = 0.560\n",
      "Epoch 73: val acc = 0.549\n",
      "Epoch 74: val acc = 0.525\n",
      "Epoch 75: val acc = 0.497\n",
      "Epoch 76: val acc = 0.486\n",
      "Epoch 77: val acc = 0.467\n",
      "Epoch 78: val acc = 0.519\n",
      "Epoch 79: val acc = 0.530\n",
      "Epoch 80: val acc = 0.511\n",
      "Epoch 81: val acc = 0.497\n",
      "Epoch 82: val acc = 0.568\n",
      "Epoch 83: val acc = 0.514\n",
      "Epoch 84: val acc = 0.489\n",
      "Epoch 85: val acc = 0.560\n",
      "Epoch 86: val acc = 0.514\n",
      "Epoch 87: val acc = 0.557\n",
      "Epoch 88: val acc = 0.590\n",
      "Epoch 89: val acc = 0.527\n",
      "Epoch 90: val acc = 0.579\n",
      "Epoch 91: val acc = 0.503\n",
      "Epoch 92: val acc = 0.522\n",
      "Epoch 93: val acc = 0.536\n",
      "Epoch 94: val acc = 0.552\n",
      "Epoch 95: val acc = 0.473\n",
      "Epoch 96: val acc = 0.541\n",
      "Epoch 97: val acc = 0.516\n",
      "Epoch 98: val acc = 0.522\n",
      "Epoch 99: val acc = 0.492\n",
      "Epoch 100: val acc = 0.552\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTMClassifier(input_size=44, hidden_size=64, num_layers=1, dropout=0.0, bidir=False).to(device)\n",
    "\n",
    "# Zone dominates class weighting, calc distro then assign man a higher waiting on the CE loss\n",
    "y_all = np.array([dataset.y[i].item() for i in range(len(dataset))], dtype=int) # Build an array of all dataset labels\n",
    "y_train = y_all[idx_train] # Slice to the training fold\n",
    "classes = np.array([0, 1], dtype=int)  # 0=Zone, 1=Man\n",
    "w = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "print(\"Class weights (Zone, Man):\", w)\n",
    "class_weights = torch.tensor(w, dtype=torch.float32, device=device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Val\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X).argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.numel()\n",
    "    print(f\"Epoch {epoch+1}: val acc = {correct/total:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4c9bf5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Zone       0.68      0.36      0.47       202\n",
      "         Man       0.50      0.79      0.61       164\n",
      "\n",
      "    accuracy                           0.55       366\n",
      "   macro avg       0.59      0.57      0.54       366\n",
      "weighted avg       0.60      0.55      0.53       366\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_true = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in val_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X).argmax(dim=1)\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_true.extend(y.cpu().numpy())\n",
    "\n",
    "print(classification_report(all_true, all_preds, target_names=[\"Zone\", \"Man\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
